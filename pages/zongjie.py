"""
Description: è¯¾ç¨‹æ€»ç»“é¡µé¢

-*- Encoding: UTF-8 -*-
@File     ï¼šzongjie.py
@Time     ï¼š2025/2/9
"""
import time
import streamlit as st
from docx import Document
from utils.utils import navigation, check_login_state, get_response_connie
from utils.styles import apply_global_styles
import re
from datetime import datetime
from langchain.memory import ConversationBufferMemory
from docx.shared import Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
import io
from configs.log_config import *


def extract_course_info(filename):
    """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸæ—¶é—´å’Œè¯¾ç¨‹åç§°"""
    try:
        # åŒ¹é…æ—¥æœŸæ—¶é—´å’Œè¯¾ç¨‹åç§°
        # ä¾‹å¦‚: "2025-02-05 08_10æ–°A_å¯¼è¯».docx"
        pattern = r"(\d{4}-\d{2}-\d{2})\s*(\d{2}[_:]\d{2})([^_\.]*)_?"
        match = re.search(pattern, filename)

        if match:
            date_str, time_str, course_name = match.groups()
            # å¤„ç†æ—¶é—´ä¸­çš„ä¸‹åˆ’çº¿
            time_str = time_str.replace('_', ':')
            # æå–è¯¾ç¨‹åç§°ï¼ˆç§»é™¤å¯èƒ½çš„å‰åç©ºæ ¼ï¼‰
            course_name = course_name.strip()
            return date_str, time_str, course_name

        # å¦‚æœæ— æ³•åŒ¹é…å®Œæ•´æ ¼å¼ï¼Œå°è¯•è‡³å°‘æå–æ—¥æœŸå’Œæ—¶é—´
        date_time_pattern = r"(\d{4}-\d{2}-\d{2})\s*(\d{2}[_:]\d{2})"
        match = re.search(date_time_pattern, filename)
        if match:
            date_str, time_str = match.groups()
            time_str = time_str.replace('_', ':')
            # ä½¿ç”¨é»˜è®¤è¯¾ç¨‹åç§°
            return date_str, time_str, "è‹±è¯­è¯¾ç¨‹"

        # å¦‚æœéƒ½æ— æ³•åŒ¹é…ï¼Œä½¿ç”¨å½“å‰æ—¶é—´å’Œé»˜è®¤è¯¾ç¨‹åç§°
        now = datetime.now()
        return now.strftime('%Y-%m-%d'), now.strftime('%H:%M'), "è‹±è¯­è¯¾ç¨‹"

    except Exception as e:
        st.error(f"æå–è¯¾ç¨‹ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        now = datetime.now()
        return now.strftime('%Y-%m-%d'), now.strftime('%H:%M'), "è‹±è¯­è¯¾ç¨‹"


def extract_chapter_overview(doc_content):
    """ä»æ–‡æ¡£å†…å®¹ä¸­æå–ç« èŠ‚é€Ÿè§ˆéƒ¨åˆ†"""
    try:
        # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„æ¢è¡Œç¬¦ç»„åˆä¸ºç»Ÿä¸€çš„æ¢è¡Œç¬¦
        doc_content = doc_content.replace('\r\n', '\n').replace('\r', '\n')

        # ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ç« èŠ‚é€Ÿè§ˆéƒ¨åˆ†
        pattern = r"ç« èŠ‚é€Ÿè§ˆ[\s\n]*?((?:[\s\S]*?(?=-\s*\d{2}:\d{2})|[\s\S]*?(?=é—®ç­”å›é¡¾)|[\s\S]*?$))"
        match = re.search(pattern, doc_content, re.DOTALL)

        if match:
            # æå–åŒ¹é…çš„å†…å®¹å¹¶æ¸…ç†
            chapter_content = match.group(1).strip()

            # ä½¿ç”¨æ–°çš„æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰æ—¶é—´æ®µå†…å®¹
            time_sections = re.findall(r'-\s*\d{2}:\d{2}.*?(?=(?:-\s*\d{2}:\d{2}|\s*$))', chapter_content, re.DOTALL)

            if time_sections:
                # åˆå¹¶æ‰€æœ‰æ—¶é—´æ®µå†…å®¹
                final_content = '\n'.join(section.strip() for section in time_sections)
                return final_content

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´æ®µï¼Œä½†å†…å®¹ä¸ä¸ºç©ºï¼Œè¿”å›æ¸…ç†åçš„å†…å®¹
            if chapter_content:
                return chapter_content

        st.warning("æœªæ‰¾åˆ°æ ‡å‡†æ ¼å¼çš„ç« èŠ‚é€Ÿè§ˆï¼Œå°è¯•æå–æ—¶é—´æ®µå†…å®¹...")

        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œç›´æ¥å°è¯•æå–æ‰€æœ‰æ—¶é—´æ®µå†…å®¹
        time_sections = re.findall(r'-\s*\d{2}:\d{2}.*?(?=(?:-\s*\d{2}:\d{2}|\s*$))', doc_content, re.DOTALL)
        if time_sections:
            return '\n'.join(section.strip() for section in time_sections)

        return None
    except Exception as e:
        st.error(f"æå–ç« èŠ‚é€Ÿè§ˆæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        return None


def generate_summary(date_str, time_str, course_name, chapter_overview):
    """è°ƒç”¨APIç”Ÿæˆè¯¾ç¨‹æ€»ç»“"""
    try:
        # æ„å»ºprompt
        prompt = f"""ä½ ç°åœ¨æ˜¯ä¸€ä¸ªæ‹¥æœ‰ä¸‰åå¹´æ•™å­¦ç»éªŒçš„åˆä¸­è‹±è¯­è€å¸ˆï¼Œä½ åˆšåˆšå®Œæˆä¸€èŠ‚è‹±è¯­è¯¾çš„æˆè¯¾ï¼Œä»¥ä¸‹æ˜¯è®°å½•çš„è¯¾å ‚æˆè®°å½•ï¼š

æ—¶é—´ï¼š{date_str} {time_str}
åç§°ï¼š{course_name}

è¯¾ç« èŠ‚é€Ÿè§ˆï¼š
{chapter_overview}

è¯·å¸®æˆ‘è¿›è¡Œæ¶¦è‰²ï¼Œä¸°å¯Œå†…å®¹ï¼ŒæŒ‰ç…§æŒ‡å®šæ ¼å¼æ’°å†™ä¸€ç¯‡ä¸“ä¸šçš„è¯¾å ‚æ€»ç»“ã€‚æ€»ç»“åŒ…æ‹¬ä¸¤éƒ¨åˆ†ä¸»è¦å†…å®¹ï¼šè¯¾ç¨‹æ¦‚è¿°/å¯¹å­¦ç”Ÿè¯¾ä¸‹å­¦ä¹ çš„å»ºè®®ã€‚è¯·æŒ‰ç…§1234ç­‰è¦ç‚¹å¯¹è¯¾å ‚æ¦‚è¿°è¿›è¡Œæç‚¼ã€‚ä¸»è¦æ€»ç»“è¯¾å ‚ä¸Šè®²æˆäº†ä»€ä¹ˆçŸ¥è¯†ï¼Œå…¶ä»–æ— å…³ç´§è¦çš„ä¸è¦æ€»ç»“ã€‚

æ€»ç»“ä¸è¦åˆ†å¤ªå¤šçº§ã€‚æŒ‡å®šæ ¼å¼ï¼š
æˆè¯¾æ—¶é—´ï¼š{date_str}
è¯¾ç¨‹åç§°ï¼š{course_name}
è¯¾ç¨‹æ€»ç»“ï¼š{{content of è¯¾ç¨‹æ€»ç»“}}
è¯¾åå»ºè®®ï¼š{{content of è¯¾åå»ºè®®}}"""

        # åˆ›å»ºå¯¹è¯è®°å¿†å¯¹è±¡
        memory = ConversationBufferMemory()
        max_retries = 3
        retry_delay = 3  # é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

        for attempt in range(max_retries):
            try:
                # è°ƒç”¨APIè·å–å“åº”
                response = get_response_connie(prompt, memory)
                if response and isinstance(response, str) and len(response.strip()) > 0:
                    # éªŒè¯å“åº”æ˜¯å¦åŒ…å«å¿…è¦çš„å†…å®¹
                    required_keywords = ["è¯¾ç¨‹æ€»ç»“", "è¯¾åå»ºè®®"]
                    if all(keyword in response for keyword in required_keywords):
                        return response

                # å¦‚æœå“åº”ä¸ç¬¦åˆè¦æ±‚ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    st.warning(f"APIå“åº”ä¸ç¬¦åˆè¦æ±‚ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{attempt + 2}æ¬¡å°è¯•...")
                    time.sleep(retry_delay)

            except Exception as e:
                if attempt < max_retries - 1:
                    st.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{attempt + 2}æ¬¡å°è¯•...")
                    time.sleep(retry_delay)
                else:
                    raise Exception(f"å¤šæ¬¡å°è¯•åAPIè°ƒç”¨ä»ç„¶å¤±è´¥ï¼š{str(e)}")

        raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„APIå“åº”")

    except Exception as e:
        st.error(f"ç”Ÿæˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        logger.error(f"ç”Ÿæˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        return None


def create_summary_document(summary_text, original_filename):
    """åˆ›å»ºæ€»ç»“æ–‡æ¡£"""
    try:
        doc = Document()

        # æ¸…ç†æ–‡æœ¬ä¸­çš„*ç¬¦å·
        cleaned_text = summary_text.replace('*', '')

        # è®¾ç½®æ–‡æ¡£æ ‡é¢˜
        title = doc.add_paragraph("è¯¾ç¨‹æ€»ç»“")
        title_format = title.runs[0]
        title_format.font.name = 'Microsoft YaHei'
        title_format.font.size = Pt(16)
        title_format.font.bold = True
        title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

        # æ·»åŠ æ€»ç»“å†…å®¹
        content = doc.add_paragraph(cleaned_text)
        content_format = content.runs[0]
        content_format.font.name = 'Microsoft YaHei'
        content_format.font.size = Pt(12)

        # ä¿å­˜åˆ°å†…å­˜ä¸­
        doc_binary = io.BytesIO()
        doc.save(doc_binary)
        doc_binary.seek(0)

        return doc_binary
    except Exception as e:
        st.error(f"åˆ›å»ºæ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        return None


def process_documents(uploaded_files):
    """å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£"""
    try:
        # ç”¨äºæ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„æ–‡æ¡£
        generated_docs = []
        failed_files = []
        total_files = len(uploaded_files)
        processed_count = 0

        for uploaded_file in uploaded_files:
            # åˆ›å»ºä¿¡æ¯æç¤ºçš„å ä½ç¬¦
            info_placeholder = st.empty()
            info_placeholder.info(f"æ­£åœ¨å¤„ç†ï¼š{uploaded_file.name} ({processed_count + 1}/{total_files})")

            # æå–æ–‡æ¡£ä¿¡æ¯
            doc = Document(uploaded_file)
            doc_content = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            date_str, time_str, course_name = extract_course_info(uploaded_file.name)
            chapter_overview = extract_chapter_overview(doc_content)

            if not chapter_overview:
                info_placeholder.empty()
                failed_files.append((uploaded_file.name, "æœªæ‰¾åˆ°ç« èŠ‚é€Ÿè§ˆéƒ¨åˆ†"))
                processed_count += 1
                continue

            # ç”Ÿæˆæ€»ç»“
            summary = generate_summary(date_str, time_str, course_name, chapter_overview)

            if not summary:
                info_placeholder.empty()
                failed_files.append((uploaded_file.name, "ç”Ÿæˆæ€»ç»“å¤±è´¥"))
                processed_count += 1
                continue

            # åˆ›å»ºæ–‡æ¡£
            doc_binary = create_summary_document(summary, uploaded_file.name)

            if not doc_binary:
                info_placeholder.empty()
                failed_files.append((uploaded_file.name, "åˆ›å»ºæ–‡æ¡£å¤±è´¥"))
                processed_count += 1
                continue

            # ä¿å­˜ç”Ÿæˆçš„æ–‡æ¡£ä¿¡æ¯
            output_filename = f"{date_str}_{course_name}_è¯¾ç¨‹æ€»ç»“.docx"
            generated_docs.append((output_filename, doc_binary))

            # æ¸…é™¤å¤„ç†ä¸­æç¤º
            info_placeholder.empty()
            processed_count += 1

        # å¤„ç†å®Œæˆåï¼Œæ˜¾ç¤ºç»“æœ
        if generated_docs:
            st.success(f"å…±å¤„ç†å®Œæˆ {len(generated_docs)} ä¸ªæ–‡ä»¶ï¼")

            # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æ¡£
            if len(generated_docs) == 1:
                filename, doc_binary = generated_docs[0]
                st.download_button(
                    label="ä¸‹è½½è¯¾ç¨‹æ€»ç»“",
                    data=doc_binary,
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    type="primary"
                )
            # å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£ï¼Œåˆ›å»ºzipæ–‡ä»¶
            else:
                import io
                import zipfile

                # åˆ›å»ºå†…å­˜ä¸­çš„zipæ–‡ä»¶
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for filename, doc_binary in generated_docs:
                        zip_file.writestr(filename, doc_binary.getvalue())

                # æä¾›zipæ–‡ä»¶ä¸‹è½½
                st.download_button(
                    label="ä¸‹è½½æ‰€æœ‰è¯¾ç¨‹æ€»ç»“",
                    data=zip_buffer.getvalue(),
                    file_name="è¯¾ç¨‹æ€»ç»“é›†.zip",
                    mime="application/zip",
                    type="primary"
                )

        # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨
        if failed_files:
            st.error("ä»¥ä¸‹æ–‡ä»¶å¤„ç†å¤±è´¥ï¼š")
            for filename, reason in failed_files:
                st.write(f"- {filename}ï¼š{reason}")

    except Exception as e:
        st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")


def course_summary():
    st.set_page_config(page_title='ATM-Cleaning', page_icon='images/favicon.png')
    apply_global_styles()
    login_state, role = check_login_state()

    if not login_state:
        error = st.error("æ‚¨è¿˜æ²¡æœ‰ç™»å½•ï¼3ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
        time.sleep(1)
        error.empty()
        error = st.error("æ‚¨è¿˜æ²¡æœ‰ç™»å½•ï¼2ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
        time.sleep(1)
        error.empty()
        st.error("æ‚¨è¿˜æ²¡æœ‰ç™»å½•ï¼1ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
        time.sleep(1)
        st.switch_page("pages/login_page.py")
    else:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šç”¨æˆ·
        current_user = st.session_state.get("logged_in_username")
        if current_user != "connie":
            error = st.error("æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤é¡µé¢ï¼3ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
            time.sleep(1)
            error.empty()
            error = st.error("æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤é¡µé¢ï¼2ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
            time.sleep(1)
            error.empty()
            st.error("æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤é¡µé¢ï¼1ç§’åè·³è½¬è‡³ç™»å½•é¡µé¢...", icon="âš ï¸")
            time.sleep(1)
            st.switch_page("pages/login_page.py")
        else:
            navigation()
            st.title("ğŸ“šè¯¾ç¨‹æ€»ç»“")
            st.divider()

            # æ·»åŠ è¯´æ˜ä¿¡æ¯
            st.info("è¯·ä¸Šä¼ æ‚¨éœ€è¦å¤„ç†çš„è®°å½•æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶åº”ä»é€šä¹‰å¬æ‚Ÿä¸­ç›´æ¥å¯¼å‡ºï¼Œä¸è¦å¯¹å¯¼å‡ºæ–‡ä»¶è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚", icon="â„¹ï¸")

            # åˆå§‹åŒ–session stateæ¥å­˜å‚¨å·²å¤„ç†çš„æ–‡ä»¶å
            if 'processed_files' not in st.session_state:
                st.session_state.processed_files = set()

            # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
            uploaded_files = st.file_uploader(
                "é€‰æ‹©è¦å¤„ç†çš„Wordæ–‡æ¡£",
                type=['docx'],
                accept_multiple_files=True,
                key='file_uploader'
            )

            if uploaded_files:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦é‡å¤
                current_files = set()
                valid_files = []

                for file in uploaded_files:
                    if file.name not in current_files and file.name not in st.session_state.processed_files:
                        current_files.add(file.name)
                        valid_files.append(file)

                if valid_files and st.button("å¼€å§‹å¤„ç†", type="primary"):
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                        process_documents(valid_files)
                        # æ·»åŠ å¤„ç†è¿‡çš„æ–‡ä»¶ååˆ°session state
                        st.session_state.processed_files.update(current_files)


if __name__ == '__main__':
    course_summary()